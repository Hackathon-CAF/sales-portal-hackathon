{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNVXn1xwP6ix"
      },
      "outputs": [],
      "source": [
        "# Instalando depend√™ncias b√°sicas\n",
        "!pip install pandas streamlit requests python-dotenv\n",
        "\n",
        "# Instalando bibliotecas para o LLM Open Source (Usaremos Transformers para carregar um modelo)\n",
        "!pip install transformers accelerate bitsandbytes sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- Seu c√≥digo de SIMULA√á√ÉO DE DADOS (mantido para garantir a execu√ß√£o) ---\n",
        "\n",
        "np.random.seed(42)\n",
        "num_pedidos = 1000\n",
        "estados = [\"SP\", \"RJ\", \"MG\", \"BA\", \"PE\", \"RS\", \"PR\", \"AM\", \"GO\"]\n",
        "\n",
        "# C√°lculo de probabilidade\n",
        "pesos_produtos = np.linspace(0.01, 0.1, 20)\n",
        "probabilidades_produtos = pesos_produtos / np.sum(pesos_produtos)\n",
        "\n",
        "data = {\n",
        "    'orderId': np.arange(1000, 1000 + num_pedidos),\n",
        "    'preco_total_item': np.round(np.random.uniform(10.0, 500.0, num_pedidos), 2),\n",
        "    'data_pedido': pd.to_datetime(pd.date_range(start='2024-01-01', periods=num_pedidos, freq='D')),\n",
        "    'status_pedido': np.random.choice([\"Completed\", \"Pending\", \"Canceled\", \"Processing\"], num_pedidos, p=[0.75, 0.15, 0.05, 0.05]),\n",
        "    'nome_cliente': [f\"Cliente_{i % 100}\" for i in range(num_pedidos)],\n",
        "    'segmento_cliente': np.random.choice([\"Gold\", \"Silver\", \"Bronze\", \"Standard\"], num_pedidos, p=[0.15, 0.25, 0.35, 0.25]),\n",
        "    'cidade': [f\"Cidade_{i % 50}\" for i in range(num_pedidos)],\n",
        "    'estado': np.random.choice(estados, num_pedidos),\n",
        "    'nome_produto': np.random.choice([f\"Produto_{i}\" for i in range(20)], num_pedidos, p=probabilidades_produtos),\n",
        "    'quantidade': np.random.randint(1, 6, num_pedidos)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Recria√ß√£o das Colunas de An√°lise\n",
        "def map_region(estado):\n",
        "    if not estado: return \"Desconhecida\"\n",
        "    estado = estado.upper()\n",
        "    if estado in [\"AC\", \"AP\", \"AM\", \"PA\", \"RO\", \"RR\", \"TO\"]: return \"Norte\"\n",
        "    elif estado in [\"AL\", \"BA\", \"CE\", \"MA\", \"PB\", \"PE\", \"PI\", \"RN\", \"SE\"]: return \"Nordeste\"\n",
        "    elif estado in [\"DF\", \"GO\", \"MT\", \"MS\"]: return \"Centro-Oeste\"\n",
        "    elif estado in [\"ES\", \"MG\", \"RJ\", \"SP\"]: return \"Sudeste\"\n",
        "    elif estado in [\"PR\", \"RS\", \"SC\"]: return \"Sul\"\n",
        "    else: return \"Desconhecida\"\n",
        "\n",
        "df['regiao'] = df['estado'].apply(map_region)\n",
        "df['mes_ano'] = df['data_pedido'].dt.to_period('M').astype(str)\n",
        "\n",
        "# --- CRIA√á√ÉO E DEFINI√á√ÉO DO df_kpi ---\n",
        "receita_total = df['preco_total_item'].sum()\n",
        "pedidos_unicos = df['orderId'].nunique()\n",
        "df_kpi = pd.DataFrame([{\n",
        "    'receita_total': receita_total,\n",
        "    'pedidos_unicos': pedidos_unicos,\n",
        "    'ticket_medio': receita_total / pedidos_unicos if pedidos_unicos > 0 else 0\n",
        "}])\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# ‚ö° TROCA DO MODELO PARA OTIMIZA√á√ÉO DA VELOCIDADE ‚ö°\n",
        "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# --- 1. Carregamento e Configura√ß√£o do LLM ---\n",
        "\n",
        "try:\n",
        "    # Tenta usar as vari√°veis globais (se j√° carregadas)\n",
        "    global tokenizer, model\n",
        "    _ = tokenizer\n",
        "    _ = model\n",
        "    print(f\"Modelo {MODEL_NAME} j√° estava carregado. Continuando...\")\n",
        "\n",
        "except NameError:\n",
        "    # Se n√£o estiverem carregadas, carrega o modelo Phi-3\n",
        "    print(f\"Carregando Modelo {MODEL_NAME}. Isso deve ser muito mais r√°pido que o Mistral 7B...\")\n",
        "    try:\n",
        "        # Usamos torch_dtype=None, confiando no dispositivo autom√°tico para otimizar\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16, # Ajuste para melhor desempenho na T4\n",
        "            trust_remote_code=True # Necess√°rio para o Phi-3\n",
        "        )\n",
        "        print(f\"Modelo {MODEL_NAME} carregado com sucesso na GPU! Espere uma infer√™ncia muito mais r√°pida.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo {MODEL_NAME}. Verifique se o runtime est√° em GPU. Erro: {e}\")\n",
        "        raise SystemExit(e) # Para a execu√ß√£o se o modelo n√£o carregar\n",
        "\n",
        "# üöÄ FUN√á√ÉO DE GERA√á√ÉO DE INSIGHTS (Compat√≠vel com Phi-3 e limpeza) üöÄ\n",
        "def generate_insights(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    # O Phi-3 usa o template de chat do Mistral/Llama\n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = encodeds.to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs,\n",
        "        max_new_tokens=1000,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
        "\n",
        "    # Limpeza: Procurar pela tag de fechamento da instru√ß√£o [/INST]\n",
        "    try:\n",
        "        start_index = response.index('[/INST]') + len('[/INST]')\n",
        "        clean_response = response[start_index:].strip()\n",
        "    except ValueError:\n",
        "        clean_response = response.strip()\n",
        "\n",
        "    return clean_response\n",
        "\n",
        "# --- 2. Prepara√ß√£o dos Dados para o Prompt ---\n",
        "\n",
        "data_summary = f\"\"\"\n",
        "## Resumo do Dataframe de Vendas:\n",
        "* **Total de Pedidos:** {df_kpi['pedidos_unicos'].iloc[0]}\n",
        "* **Receita Total:** R$ {df_kpi['receita_total'].iloc[0]:,.2f}\n",
        "* **Ticket M√©dio Global:** R$ {df_kpi['ticket_medio'].iloc[0]:,.2f}\n",
        "\n",
        "## Amostra de Dados (df.head()):\n",
        "{df.head().to_markdown(index=False)}\n",
        "\n",
        "## Agregados Relevantes:\n",
        "* **Receita por Status:**\n",
        "{df.groupby('status_pedido')['preco_total_item'].sum().sort_values(ascending=False).to_markdown()}\n",
        "\n",
        "* **Receita por Regi√£o (Top 3):**\n",
        "{df.groupby('regiao')['preco_total_item'].sum().nlargest(3).to_markdown()}\n",
        "\n",
        "* **Produtos Mais Vendidos (Top 5 em Receita):**\n",
        "{df.groupby('nome_produto')['preco_total_item'].sum().nlargest(5).to_markdown()}\n",
        "\n",
        "* **Segmento de Cliente (Receita Total):**\n",
        "{df.groupby('segmento_cliente')['preco_total_item'].sum().sort_values(ascending=False).to_markdown()}\n",
        "\"\"\"\n",
        "\n",
        "# --- 3. Prompt S√™nior para Gera√ß√£o de Insights (Otimizado para Portugu√™s e Feedback Acion√°vel) ---\n",
        "analyst_prompt = f\"\"\"\n",
        "Voc√™ √© um Analista de Dados S√™nior e fluente em Portugu√™s do Brasil.\n",
        "Sua tarefa √© analisar os dados de vendas fornecidos abaixo e produzir uma an√°lise focada em **Tomada de Decis√£o**.\n",
        "\n",
        "Instru√ß√µes Cruciais:\n",
        "1.  **IDIOMA:** A resposta DEVE ser **100% em Portugu√™s do Brasil**.\n",
        "2.  **FORMATO:** Gere 5 insights essenciais, formatados como uma lista numerada.\n",
        "3.  **ESTRUTURA:** Cada ponto deve seguir esta estrutura EXATA, utilizando negrito para destaque:\n",
        "    * **Insight Chave:** [O fato principal e a evid√™ncia num√©rica (R$) extra√≠da dos dados.]\n",
        "    * **Recomenda√ß√£o Acion√°vel:** [O que a equipe de vendas/marketing deve fazer com base no insight.]\n",
        "\n",
        "4.  **REGRAS R√çGIDAS DE FORMATA√á√ÉO:**\n",
        "    * N√ÉO use as palavras 'Analyze', 'Analysis', 'Recommendation' ou 'Recomenda√ß√£o' como cabe√ßalhos.\n",
        "    * N√ÉO use marcadores de lista internos (como bullet points). Mantenha a resposta concisa.\n",
        "    * Use o formato monet√°rio (R$) corretamente.\n",
        "\n",
        "--- DADOS PARA AN√ÅLISE ---\n",
        "{data_summary}\n",
        "--- FIM DOS DADOS ---\n",
        "\n",
        "## üìä An√°lise de 5 Pontos com A√ß√µes Recomendadas:\n",
        "\"\"\"\n",
        "\n",
        "# --- 4. Execu√ß√£o do Agente ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ü§ñ AGENTE DE INSIGHTS DE IA EM EXECU√á√ÉO...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Chamar a fun√ß√£o de gera√ß√£o de insights\n",
        "insights = generate_insights(analyst_prompt)\n",
        "\n",
        "# --- 5. Exibi√ß√£o dos Resultados ---\n",
        "print(\"\\n\" + \"#\"*50)\n",
        "display(Markdown(\"## ‚ú® Insights Gerados pelo Modelo Phi-3 Mini\"))\n",
        "display(Markdown(insights))\n",
        "print(\"#\"*50)"
      ],
      "metadata": {
        "id": "GeiFT64uTIvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}